{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8bd95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets, transforms\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b880a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a212b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.labels = pd.read_csv(csv_file).sample(5000)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding_path = os.path.join(self.root_dir, self.labels.iloc[idx,0])\n",
    "        embedding = torch.load(embedding_path)[0,0,1:,:]\n",
    "        label = literal_eval(self.labels.iloc[idx, 1])\n",
    "        if self.transform:\n",
    "            embedding = self.transform(embedding)\n",
    "        return embedding, label, embedding_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "051648b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig = SmallDataset(csv_file='../data/rmsd_dataset.csv',\n",
    "                                    root_dir='/mnt/nasdata/vera/msa_transformer_embeddings/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e9bc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m all_residues \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset_orig)):\n\u001b[0;32m----> 7\u001b[0m     feature, label, \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m dataset_orig[i]\n\u001b[1;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m residue \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(feature\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m      9\u001b[0m         all_features\u001b[39m.\u001b[39mappend(feature[residue]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mSmallDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m     11\u001b[0m     embedding_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels\u001b[39m.\u001b[39miloc[idx,\u001b[39m0\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m     embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(embedding_path)[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m:,:]\n\u001b[1;32m     13\u001b[0m     label \u001b[39m=\u001b[39m literal_eval(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels\u001b[39m.\u001b[39miloc[idx, \u001b[39m1\u001b[39m])\n\u001b[1;32m     14\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/serialization.py:811\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    810\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 811\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    812\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    813\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/serialization.py:1174\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1172\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1173\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1174\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1176\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1178\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/serialization.py:1144\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1143\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1144\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1146\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/serialization.py:1114\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   1112\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1114\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1115\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m   1118\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1119\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1120\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_features = []\n",
    "all_labels = []\n",
    "all_pdb_ids = []\n",
    "all_residues = []\n",
    "\n",
    "for i in range(len(dataset_orig)):\n",
    "    feature, label, id = dataset_orig[i]\n",
    "    for residue in range(feature.shape[0]):\n",
    "        all_features.append(feature[residue].cpu().numpy())\n",
    "        all_labels.append(label[residue])\n",
    "        all_pdb_ids.append(id)\n",
    "        all_residues.append(residue)\n",
    "    print(i)\n",
    "\n",
    "print(len(all_features), len(all_labels))\n",
    "\n",
    "feature_tensor = torch.from_numpy(np.stack(all_features))\n",
    "torch.save(feature_tensor, '../data/5k_protein_dataset/5k_protein_tensor.pt')\n",
    "label_tuples = list(zip(all_pdb_ids,all_residues,all_labels))\n",
    "labels_df = pd.DataFrame(label_tuples, columns=['pdb','residue','rmsd'])\n",
    "labels_df.to_csv('../data/5k_protein_dataset/5k_protein_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6bac1c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([112801, 768])\n",
      "(112801, 4)\n"
     ]
    }
   ],
   "source": [
    "feature_tensor = torch.load('../data/500_protein_dataset/500_protein_tensor.pt')\n",
    "labels_df = pd.read_csv('../data/500_protein_dataset/500_protein_labels.csv')\n",
    "print(feature_tensor.shape)\n",
    "print(labels_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2c027d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.18%\n",
      "13.67%\n",
      "10.85%\n",
      "9.24%\n",
      "8.16%\n",
      "7.26%\n",
      "6.56%\n",
      "5.94%\n",
      "5.44%\n"
     ]
    }
   ],
   "source": [
    "# Iterate over different rmsd thresholds and print the percentage of the dataset that is selected\n",
    "for i in range(1,10):\n",
    "    select = labels_df['rmsd'] > i/5\n",
    "    select_df = labels_df[select]\n",
    "    full_length = len(labels_df)\n",
    "    select_length = len(select_df)\n",
    "    print(\"{:.2%}\".format(select_length/full_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f1d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSD_THRESHOLD = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea46823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallDataset500(Dataset):\n",
    "    def __init__(self, pt_file, label_csv, transform=None):\n",
    "        self.features = torch.load(pt_file)\n",
    "        self.labels = pd.read_csv(label_csv)\n",
    "        self.labels = self.labels.drop(columns=['Unnamed: 0'])\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        label = int(self.labels.loc[idx, 'rmsd'] >= RMSD_THRESHOLD)\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ce62784",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SmallDataset500(pt_file='../data/500_protein_dataset/500_protein_tensor.pt', label_csv='../data/500_protein_dataset/500_protein_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d7d679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 1\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n",
      "torch.Size([768]) 0\n"
     ]
    }
   ],
   "source": [
    "# Test some of the data\n",
    "for i in range(10, 50):\n",
    "    feature, label = dataset[i]\n",
    "    print(feature.shape, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb393028",
   "metadata": {},
   "source": [
    "## The following code is written by copilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63127189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets.\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afce6e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90240 22561\n"
     ]
    }
   ],
   "source": [
    "# Test the split\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5032e19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 768]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Create the dataloaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Test the dataloaders\n",
    "for i, (features, labels) in enumerate(train_loader):\n",
    "    print(features.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1d828c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([[-1.3566, -0.0953,  1.8045,  ..., -0.9411,  0.4137,  0.0075],\n",
      "        [-0.6833, -0.6870, -0.5969,  ..., -0.9105,  0.3010, -0.3303],\n",
      "        [-0.8243, -0.1624, -0.6868,  ...,  0.6887, -0.3415, -0.8316],\n",
      "        ...,\n",
      "        [-0.7919, -1.0166,  0.2467,  ..., -1.2035, -0.6025, -0.1938],\n",
      "        [-0.8047, -0.3926,  0.5995,  ..., -1.2277,  1.0002,  0.1274],\n",
      "        [-1.1114, -0.7191,  0.0479,  ..., -0.2406,  0.2433, -1.1319]])\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7091e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a binary classifier with five hidden layers\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30f27156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved model using regularization and batch normalization\n",
    "class ImprovedClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(256)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        x = self.relu(self.batchnorm1(self.fc1(x)))\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.relu(self.batchnorm2(self.fc2(x)))\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.relu(self.batchnorm3(self.fc3(x)))\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.relu(self.batchnorm4(self.fc4(x)))\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f49ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/705], Loss: 0.6112\n",
      "Epoch [1/50], Step [200/705], Loss: 0.5752\n",
      "Epoch [1/50], Step [300/705], Loss: 0.5994\n",
      "Epoch [1/50], Step [400/705], Loss: 0.5520\n",
      "Epoch [1/50], Step [500/705], Loss: 0.5562\n",
      "Epoch [1/50], Step [600/705], Loss: 0.5139\n",
      "Epoch [1/50], Step [700/705], Loss: 0.5334\n",
      "Epoch [2/50], Step [100/705], Loss: 0.5200\n",
      "Epoch [2/50], Step [200/705], Loss: 0.5607\n",
      "Epoch [2/50], Step [300/705], Loss: 0.5111\n",
      "Epoch [2/50], Step [400/705], Loss: 0.4891\n",
      "Epoch [2/50], Step [500/705], Loss: 0.5036\n",
      "Epoch [2/50], Step [600/705], Loss: 0.5174\n",
      "Epoch [2/50], Step [700/705], Loss: 0.5080\n",
      "Epoch [3/50], Step [100/705], Loss: 0.5309\n",
      "Epoch [3/50], Step [200/705], Loss: 0.5463\n",
      "Epoch [3/50], Step [300/705], Loss: 0.5227\n",
      "Epoch [3/50], Step [400/705], Loss: 0.5331\n",
      "Epoch [3/50], Step [500/705], Loss: 0.5009\n",
      "Epoch [3/50], Step [600/705], Loss: 0.5187\n",
      "Epoch [3/50], Step [700/705], Loss: 0.5443\n",
      "Epoch [4/50], Step [100/705], Loss: 0.5066\n",
      "Epoch [4/50], Step [200/705], Loss: 0.4936\n",
      "Epoch [4/50], Step [300/705], Loss: 0.5438\n",
      "Epoch [4/50], Step [400/705], Loss: 0.4790\n",
      "Epoch [4/50], Step [500/705], Loss: 0.5191\n",
      "Epoch [4/50], Step [600/705], Loss: 0.4659\n",
      "Epoch [4/50], Step [700/705], Loss: 0.4937\n",
      "Epoch [5/50], Step [100/705], Loss: 0.5330\n",
      "Epoch [5/50], Step [200/705], Loss: 0.4903\n",
      "Epoch [5/50], Step [300/705], Loss: 0.4603\n",
      "Epoch [5/50], Step [400/705], Loss: 0.4756\n",
      "Epoch [5/50], Step [500/705], Loss: 0.5177\n",
      "Epoch [5/50], Step [600/705], Loss: 0.4988\n",
      "Epoch [5/50], Step [700/705], Loss: 0.4927\n",
      "Epoch [6/50], Step [100/705], Loss: 0.4908\n",
      "Epoch [6/50], Step [200/705], Loss: 0.4534\n",
      "Epoch [6/50], Step [300/705], Loss: 0.4853\n",
      "Epoch [6/50], Step [400/705], Loss: 0.4679\n",
      "Epoch [6/50], Step [500/705], Loss: 0.4678\n",
      "Epoch [6/50], Step [600/705], Loss: 0.5069\n",
      "Epoch [6/50], Step [700/705], Loss: 0.4603\n",
      "Epoch [7/50], Step [100/705], Loss: 0.5172\n",
      "Epoch [7/50], Step [200/705], Loss: 0.4993\n",
      "Epoch [7/50], Step [300/705], Loss: 0.5180\n",
      "Epoch [7/50], Step [400/705], Loss: 0.5023\n",
      "Epoch [7/50], Step [500/705], Loss: 0.4958\n",
      "Epoch [7/50], Step [600/705], Loss: 0.4868\n",
      "Epoch [7/50], Step [700/705], Loss: 0.4759\n",
      "Epoch [8/50], Step [100/705], Loss: 0.4839\n",
      "Epoch [8/50], Step [200/705], Loss: 0.4644\n",
      "Epoch [8/50], Step [300/705], Loss: 0.4420\n",
      "Epoch [8/50], Step [400/705], Loss: 0.4132\n",
      "Epoch [8/50], Step [500/705], Loss: 0.4525\n",
      "Epoch [8/50], Step [600/705], Loss: 0.4523\n",
      "Epoch [8/50], Step [700/705], Loss: 0.4540\n",
      "Epoch [9/50], Step [100/705], Loss: 0.4959\n",
      "Epoch [9/50], Step [200/705], Loss: 0.4774\n",
      "Epoch [9/50], Step [300/705], Loss: 0.5133\n",
      "Epoch [9/50], Step [400/705], Loss: 0.4774\n",
      "Epoch [9/50], Step [500/705], Loss: 0.4416\n",
      "Epoch [9/50], Step [600/705], Loss: 0.4539\n",
      "Epoch [9/50], Step [700/705], Loss: 0.5568\n",
      "Epoch [10/50], Step [100/705], Loss: 0.4621\n",
      "Epoch [10/50], Step [200/705], Loss: 0.4694\n",
      "Epoch [10/50], Step [300/705], Loss: 0.5036\n",
      "Epoch [10/50], Step [400/705], Loss: 0.5006\n",
      "Epoch [10/50], Step [500/705], Loss: 0.4334\n",
      "Epoch [10/50], Step [600/705], Loss: 0.4613\n",
      "Epoch [10/50], Step [700/705], Loss: 0.4220\n",
      "Epoch [11/50], Step [100/705], Loss: 0.4713\n",
      "Epoch [11/50], Step [200/705], Loss: 0.4344\n",
      "Epoch [11/50], Step [300/705], Loss: 0.4625\n",
      "Epoch [11/50], Step [400/705], Loss: 0.4261\n",
      "Epoch [11/50], Step [500/705], Loss: 0.4804\n",
      "Epoch [11/50], Step [600/705], Loss: 0.4269\n",
      "Epoch [11/50], Step [700/705], Loss: 0.4325\n",
      "Epoch [12/50], Step [100/705], Loss: 0.4539\n",
      "Epoch [12/50], Step [200/705], Loss: 0.4509\n",
      "Epoch [12/50], Step [300/705], Loss: 0.4946\n",
      "Epoch [12/50], Step [400/705], Loss: 0.4211\n",
      "Epoch [12/50], Step [500/705], Loss: 0.4920\n",
      "Epoch [12/50], Step [600/705], Loss: 0.5199\n",
      "Epoch [12/50], Step [700/705], Loss: 0.4608\n",
      "Epoch [13/50], Step [100/705], Loss: 0.4691\n",
      "Epoch [13/50], Step [200/705], Loss: 0.4258\n",
      "Epoch [13/50], Step [300/705], Loss: 0.4772\n",
      "Epoch [13/50], Step [400/705], Loss: 0.4828\n",
      "Epoch [13/50], Step [500/705], Loss: 0.4356\n",
      "Epoch [13/50], Step [600/705], Loss: 0.4055\n",
      "Epoch [13/50], Step [700/705], Loss: 0.4137\n",
      "Epoch [14/50], Step [100/705], Loss: 0.4144\n",
      "Epoch [14/50], Step [200/705], Loss: 0.4798\n",
      "Epoch [14/50], Step [300/705], Loss: 0.4224\n",
      "Epoch [14/50], Step [400/705], Loss: 0.4610\n",
      "Epoch [14/50], Step [500/705], Loss: 0.4253\n",
      "Epoch [14/50], Step [600/705], Loss: 0.4037\n",
      "Epoch [14/50], Step [700/705], Loss: 0.4290\n",
      "Epoch [15/50], Step [100/705], Loss: 0.4611\n",
      "Epoch [15/50], Step [200/705], Loss: 0.4202\n",
      "Epoch [15/50], Step [300/705], Loss: 0.4358\n",
      "Epoch [15/50], Step [400/705], Loss: 0.4390\n",
      "Epoch [15/50], Step [500/705], Loss: 0.4030\n",
      "Epoch [15/50], Step [600/705], Loss: 0.3949\n",
      "Epoch [15/50], Step [700/705], Loss: 0.4497\n",
      "Epoch [16/50], Step [100/705], Loss: 0.4832\n",
      "Epoch [16/50], Step [200/705], Loss: 0.4231\n",
      "Epoch [16/50], Step [300/705], Loss: 0.4558\n",
      "Epoch [16/50], Step [400/705], Loss: 0.3968\n",
      "Epoch [16/50], Step [500/705], Loss: 0.4744\n",
      "Epoch [16/50], Step [600/705], Loss: 0.4313\n",
      "Epoch [16/50], Step [700/705], Loss: 0.4790\n",
      "Epoch [17/50], Step [100/705], Loss: 0.4306\n",
      "Epoch [17/50], Step [200/705], Loss: 0.4175\n",
      "Epoch [17/50], Step [300/705], Loss: 0.3981\n",
      "Epoch [17/50], Step [400/705], Loss: 0.4923\n",
      "Epoch [17/50], Step [500/705], Loss: 0.4295\n",
      "Epoch [17/50], Step [600/705], Loss: 0.4442\n",
      "Epoch [17/50], Step [700/705], Loss: 0.4280\n",
      "Epoch [18/50], Step [100/705], Loss: 0.4095\n",
      "Epoch [18/50], Step [200/705], Loss: 0.4299\n",
      "Epoch [18/50], Step [300/705], Loss: 0.4305\n",
      "Epoch [18/50], Step [400/705], Loss: 0.4339\n",
      "Epoch [18/50], Step [500/705], Loss: 0.4325\n",
      "Epoch [18/50], Step [600/705], Loss: 0.4172\n",
      "Epoch [18/50], Step [700/705], Loss: 0.4507\n",
      "Epoch [19/50], Step [100/705], Loss: 0.4579\n",
      "Epoch [19/50], Step [200/705], Loss: 0.4220\n",
      "Epoch [19/50], Step [300/705], Loss: 0.3903\n",
      "Epoch [19/50], Step [400/705], Loss: 0.4208\n",
      "Epoch [19/50], Step [500/705], Loss: 0.4260\n",
      "Epoch [19/50], Step [600/705], Loss: 0.4520\n",
      "Epoch [19/50], Step [700/705], Loss: 0.4480\n",
      "Epoch [20/50], Step [100/705], Loss: 0.4235\n",
      "Epoch [20/50], Step [200/705], Loss: 0.4161\n",
      "Epoch [20/50], Step [300/705], Loss: 0.4352\n",
      "Epoch [20/50], Step [400/705], Loss: 0.4462\n",
      "Epoch [20/50], Step [500/705], Loss: 0.4287\n",
      "Epoch [20/50], Step [600/705], Loss: 0.4550\n",
      "Epoch [20/50], Step [700/705], Loss: 0.4405\n",
      "Epoch [21/50], Step [100/705], Loss: 0.4478\n",
      "Epoch [21/50], Step [200/705], Loss: 0.4303\n",
      "Epoch [21/50], Step [300/705], Loss: 0.4166\n",
      "Epoch [21/50], Step [400/705], Loss: 0.4142\n",
      "Epoch [21/50], Step [500/705], Loss: 0.4311\n",
      "Epoch [21/50], Step [600/705], Loss: 0.4240\n",
      "Epoch [21/50], Step [700/705], Loss: 0.4270\n",
      "Epoch [22/50], Step [100/705], Loss: 0.3888\n",
      "Epoch [22/50], Step [200/705], Loss: 0.3984\n",
      "Epoch [22/50], Step [300/705], Loss: 0.4666\n",
      "Epoch [22/50], Step [400/705], Loss: 0.4316\n",
      "Epoch [22/50], Step [500/705], Loss: 0.4798\n",
      "Epoch [22/50], Step [600/705], Loss: 0.4466\n",
      "Epoch [22/50], Step [700/705], Loss: 0.4015\n",
      "Epoch [23/50], Step [100/705], Loss: 0.4106\n",
      "Epoch [23/50], Step [200/705], Loss: 0.4687\n",
      "Epoch [23/50], Step [300/705], Loss: 0.3931\n",
      "Epoch [23/50], Step [400/705], Loss: 0.4473\n",
      "Epoch [23/50], Step [500/705], Loss: 0.4115\n",
      "Epoch [23/50], Step [600/705], Loss: 0.4358\n",
      "Epoch [23/50], Step [700/705], Loss: 0.4440\n",
      "Epoch [24/50], Step [100/705], Loss: 0.4148\n",
      "Epoch [24/50], Step [200/705], Loss: 0.4476\n",
      "Epoch [24/50], Step [300/705], Loss: 0.3843\n",
      "Epoch [24/50], Step [400/705], Loss: 0.4134\n",
      "Epoch [24/50], Step [500/705], Loss: 0.4231\n",
      "Epoch [24/50], Step [600/705], Loss: 0.4282\n",
      "Epoch [24/50], Step [700/705], Loss: 0.4505\n",
      "Epoch [25/50], Step [100/705], Loss: 0.3926\n",
      "Epoch [25/50], Step [200/705], Loss: 0.4061\n",
      "Epoch [25/50], Step [300/705], Loss: 0.4270\n",
      "Epoch [25/50], Step [400/705], Loss: 0.4138\n",
      "Epoch [25/50], Step [500/705], Loss: 0.4051\n",
      "Epoch [25/50], Step [600/705], Loss: 0.4089\n",
      "Epoch [25/50], Step [700/705], Loss: 0.4054\n",
      "Epoch [26/50], Step [100/705], Loss: 0.4049\n",
      "Epoch [26/50], Step [200/705], Loss: 0.3579\n",
      "Epoch [26/50], Step [300/705], Loss: 0.3866\n",
      "Epoch [26/50], Step [400/705], Loss: 0.4472\n",
      "Epoch [26/50], Step [500/705], Loss: 0.4436\n",
      "Epoch [26/50], Step [600/705], Loss: 0.4143\n",
      "Epoch [26/50], Step [700/705], Loss: 0.4082\n",
      "Epoch [27/50], Step [100/705], Loss: 0.4147\n",
      "Epoch [27/50], Step [200/705], Loss: 0.4564\n",
      "Epoch [27/50], Step [300/705], Loss: 0.4366\n",
      "Epoch [27/50], Step [400/705], Loss: 0.3733\n",
      "Epoch [27/50], Step [500/705], Loss: 0.4919\n",
      "Epoch [27/50], Step [600/705], Loss: 0.4061\n",
      "Epoch [27/50], Step [700/705], Loss: 0.4181\n",
      "Epoch [28/50], Step [100/705], Loss: 0.4280\n",
      "Epoch [28/50], Step [200/705], Loss: 0.4162\n",
      "Epoch [28/50], Step [300/705], Loss: 0.4247\n",
      "Epoch [28/50], Step [400/705], Loss: 0.4081\n",
      "Epoch [28/50], Step [500/705], Loss: 0.4022\n",
      "Epoch [28/50], Step [600/705], Loss: 0.4687\n",
      "Epoch [28/50], Step [700/705], Loss: 0.4414\n",
      "Epoch [29/50], Step [100/705], Loss: 0.4545\n",
      "Epoch [29/50], Step [200/705], Loss: 0.4424\n",
      "Epoch [29/50], Step [300/705], Loss: 0.3877\n",
      "Epoch [29/50], Step [400/705], Loss: 0.4322\n",
      "Epoch [29/50], Step [500/705], Loss: 0.4339\n",
      "Epoch [29/50], Step [600/705], Loss: 0.4349\n",
      "Epoch [29/50], Step [700/705], Loss: 0.4085\n",
      "Epoch [30/50], Step [100/705], Loss: 0.4005\n",
      "Epoch [30/50], Step [200/705], Loss: 0.4242\n",
      "Epoch [30/50], Step [300/705], Loss: 0.4128\n",
      "Epoch [30/50], Step [400/705], Loss: 0.4329\n",
      "Epoch [30/50], Step [500/705], Loss: 0.3751\n",
      "Epoch [30/50], Step [600/705], Loss: 0.3818\n",
      "Epoch [30/50], Step [700/705], Loss: 0.4219\n",
      "Epoch [31/50], Step [100/705], Loss: 0.4143\n",
      "Epoch [31/50], Step [200/705], Loss: 0.4043\n",
      "Epoch [31/50], Step [300/705], Loss: 0.4423\n",
      "Epoch [31/50], Step [400/705], Loss: 0.4132\n",
      "Epoch [31/50], Step [500/705], Loss: 0.4463\n",
      "Epoch [31/50], Step [600/705], Loss: 0.4598\n",
      "Epoch [31/50], Step [700/705], Loss: 0.4173\n",
      "Epoch [32/50], Step [100/705], Loss: 0.4603\n",
      "Epoch [32/50], Step [200/705], Loss: 0.4538\n",
      "Epoch [32/50], Step [300/705], Loss: 0.3546\n",
      "Epoch [32/50], Step [400/705], Loss: 0.4637\n",
      "Epoch [32/50], Step [500/705], Loss: 0.4103\n",
      "Epoch [32/50], Step [600/705], Loss: 0.4058\n",
      "Epoch [32/50], Step [700/705], Loss: 0.3862\n",
      "Epoch [33/50], Step [100/705], Loss: 0.4246\n",
      "Epoch [33/50], Step [200/705], Loss: 0.4362\n",
      "Epoch [33/50], Step [300/705], Loss: 0.3668\n",
      "Epoch [33/50], Step [400/705], Loss: 0.4308\n",
      "Epoch [33/50], Step [500/705], Loss: 0.4264\n",
      "Epoch [33/50], Step [600/705], Loss: 0.4205\n",
      "Epoch [33/50], Step [700/705], Loss: 0.4497\n",
      "Epoch [34/50], Step [100/705], Loss: 0.4285\n",
      "Epoch [34/50], Step [200/705], Loss: 0.3872\n",
      "Epoch [34/50], Step [300/705], Loss: 0.3449\n",
      "Epoch [34/50], Step [400/705], Loss: 0.4409\n",
      "Epoch [34/50], Step [500/705], Loss: 0.4185\n",
      "Epoch [34/50], Step [600/705], Loss: 0.4310\n",
      "Epoch [34/50], Step [700/705], Loss: 0.4533\n",
      "Epoch [35/50], Step [100/705], Loss: 0.4166\n",
      "Epoch [35/50], Step [200/705], Loss: 0.4066\n",
      "Epoch [35/50], Step [300/705], Loss: 0.3882\n",
      "Epoch [35/50], Step [400/705], Loss: 0.4219\n",
      "Epoch [35/50], Step [500/705], Loss: 0.3969\n",
      "Epoch [35/50], Step [600/705], Loss: 0.3827\n",
      "Epoch [35/50], Step [700/705], Loss: 0.4472\n",
      "Epoch [36/50], Step [100/705], Loss: 0.3991\n",
      "Epoch [36/50], Step [200/705], Loss: 0.3907\n",
      "Epoch [36/50], Step [300/705], Loss: 0.4033\n",
      "Epoch [36/50], Step [400/705], Loss: 0.3640\n",
      "Epoch [36/50], Step [500/705], Loss: 0.4497\n",
      "Epoch [36/50], Step [600/705], Loss: 0.4400\n",
      "Epoch [36/50], Step [700/705], Loss: 0.3960\n",
      "Epoch [37/50], Step [100/705], Loss: 0.4249\n",
      "Epoch [37/50], Step [200/705], Loss: 0.3858\n",
      "Epoch [37/50], Step [300/705], Loss: 0.3992\n",
      "Epoch [37/50], Step [400/705], Loss: 0.4169\n",
      "Epoch [37/50], Step [500/705], Loss: 0.3979\n",
      "Epoch [37/50], Step [600/705], Loss: 0.3739\n",
      "Epoch [37/50], Step [700/705], Loss: 0.4479\n",
      "Epoch [38/50], Step [100/705], Loss: 0.3967\n",
      "Epoch [38/50], Step [200/705], Loss: 0.3713\n",
      "Epoch [38/50], Step [300/705], Loss: 0.3683\n",
      "Epoch [38/50], Step [400/705], Loss: 0.3825\n",
      "Epoch [38/50], Step [500/705], Loss: 0.4131\n",
      "Epoch [38/50], Step [600/705], Loss: 0.3784\n",
      "Epoch [38/50], Step [700/705], Loss: 0.3848\n",
      "Epoch [39/50], Step [100/705], Loss: 0.3956\n",
      "Epoch [39/50], Step [200/705], Loss: 0.4170\n",
      "Epoch [39/50], Step [300/705], Loss: 0.4303\n",
      "Epoch [39/50], Step [400/705], Loss: 0.4064\n",
      "Epoch [39/50], Step [500/705], Loss: 0.4046\n",
      "Epoch [39/50], Step [600/705], Loss: 0.4290\n",
      "Epoch [39/50], Step [700/705], Loss: 0.4191\n",
      "Epoch [40/50], Step [100/705], Loss: 0.4173\n",
      "Epoch [40/50], Step [200/705], Loss: 0.4124\n",
      "Epoch [40/50], Step [300/705], Loss: 0.3973\n",
      "Epoch [40/50], Step [400/705], Loss: 0.4316\n",
      "Epoch [40/50], Step [500/705], Loss: 0.4180\n",
      "Epoch [40/50], Step [600/705], Loss: 0.3884\n",
      "Epoch [40/50], Step [700/705], Loss: 0.3643\n",
      "Epoch [41/50], Step [100/705], Loss: 0.3906\n",
      "Epoch [41/50], Step [200/705], Loss: 0.3696\n",
      "Epoch [41/50], Step [300/705], Loss: 0.3998\n",
      "Epoch [41/50], Step [400/705], Loss: 0.4090\n",
      "Epoch [41/50], Step [500/705], Loss: 0.3872\n",
      "Epoch [41/50], Step [600/705], Loss: 0.3996\n",
      "Epoch [41/50], Step [700/705], Loss: 0.4168\n",
      "Epoch [42/50], Step [100/705], Loss: 0.4314\n",
      "Epoch [42/50], Step [200/705], Loss: 0.4081\n",
      "Epoch [42/50], Step [300/705], Loss: 0.3806\n",
      "Epoch [42/50], Step [400/705], Loss: 0.3598\n",
      "Epoch [42/50], Step [500/705], Loss: 0.3906\n",
      "Epoch [42/50], Step [600/705], Loss: 0.3961\n",
      "Epoch [42/50], Step [700/705], Loss: 0.3909\n",
      "Epoch [43/50], Step [100/705], Loss: 0.4160\n",
      "Epoch [43/50], Step [200/705], Loss: 0.3991\n",
      "Epoch [43/50], Step [300/705], Loss: 0.3809\n",
      "Epoch [43/50], Step [400/705], Loss: 0.3941\n",
      "Epoch [43/50], Step [500/705], Loss: 0.3837\n",
      "Epoch [43/50], Step [600/705], Loss: 0.3881\n",
      "Epoch [43/50], Step [700/705], Loss: 0.3989\n",
      "Epoch [44/50], Step [100/705], Loss: 0.4008\n",
      "Epoch [44/50], Step [200/705], Loss: 0.3756\n",
      "Epoch [44/50], Step [300/705], Loss: 0.4277\n",
      "Epoch [44/50], Step [400/705], Loss: 0.3921\n",
      "Epoch [44/50], Step [500/705], Loss: 0.3724\n",
      "Epoch [44/50], Step [600/705], Loss: 0.3919\n",
      "Epoch [44/50], Step [700/705], Loss: 0.4330\n",
      "Epoch [45/50], Step [100/705], Loss: 0.3992\n",
      "Epoch [45/50], Step [200/705], Loss: 0.3862\n",
      "Epoch [45/50], Step [300/705], Loss: 0.4304\n",
      "Epoch [45/50], Step [400/705], Loss: 0.4064\n",
      "Epoch [45/50], Step [500/705], Loss: 0.3720\n",
      "Epoch [45/50], Step [600/705], Loss: 0.3902\n",
      "Epoch [45/50], Step [700/705], Loss: 0.3751\n",
      "Epoch [46/50], Step [100/705], Loss: 0.4168\n",
      "Epoch [46/50], Step [200/705], Loss: 0.3704\n",
      "Epoch [46/50], Step [300/705], Loss: 0.4120\n",
      "Epoch [46/50], Step [400/705], Loss: 0.3916\n",
      "Epoch [46/50], Step [500/705], Loss: 0.4017\n",
      "Epoch [46/50], Step [600/705], Loss: 0.3944\n",
      "Epoch [46/50], Step [700/705], Loss: 0.4059\n",
      "Epoch [47/50], Step [100/705], Loss: 0.4033\n",
      "Epoch [47/50], Step [200/705], Loss: 0.3760\n",
      "Epoch [47/50], Step [300/705], Loss: 0.4061\n",
      "Epoch [47/50], Step [400/705], Loss: 0.4318\n",
      "Epoch [47/50], Step [500/705], Loss: 0.4475\n",
      "Epoch [47/50], Step [600/705], Loss: 0.4267\n",
      "Epoch [47/50], Step [700/705], Loss: 0.3762\n",
      "Epoch [48/50], Step [100/705], Loss: 0.4067\n",
      "Epoch [48/50], Step [200/705], Loss: 0.4083\n",
      "Epoch [48/50], Step [300/705], Loss: 0.3827\n",
      "Epoch [48/50], Step [400/705], Loss: 0.3682\n",
      "Epoch [48/50], Step [500/705], Loss: 0.4015\n",
      "Epoch [48/50], Step [600/705], Loss: 0.3812\n",
      "Epoch [48/50], Step [700/705], Loss: 0.3962\n",
      "Epoch [49/50], Step [100/705], Loss: 0.4061\n",
      "Epoch [49/50], Step [200/705], Loss: 0.3876\n",
      "Epoch [49/50], Step [300/705], Loss: 0.4246\n",
      "Epoch [49/50], Step [400/705], Loss: 0.3860\n",
      "Epoch [49/50], Step [500/705], Loss: 0.4425\n",
      "Epoch [49/50], Step [600/705], Loss: 0.4308\n",
      "Epoch [49/50], Step [700/705], Loss: 0.3715\n",
      "Epoch [50/50], Step [100/705], Loss: 0.4142\n",
      "Epoch [50/50], Step [200/705], Loss: 0.3695\n",
      "Epoch [50/50], Step [300/705], Loss: 0.3864\n",
      "Epoch [50/50], Step [400/705], Loss: 0.4115\n",
      "Epoch [50/50], Step [500/705], Loss: 0.3820\n",
      "Epoch [50/50], Step [600/705], Loss: 0.3969\n",
      "Epoch [50/50], Step [700/705], Loss: 0.3785\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = SimpleClassifier()\n",
    "model = ImprovedClassifier()\n",
    "model.cuda()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        # Forward pass\n",
    "        outputs = model(features, training=True)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1b13172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy:   93 %\n",
      "Test set accuracy:    82 %\n",
      "Test set precision:   62 %\n",
      "Test set recall:      44 %\n",
      "Test set F1 score     51 %\n"
     ]
    }
   ],
   "source": [
    "# Test the accuracy of the model on the train set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Train set accuracy:   %d %%' % (100 * correct / total))\n",
    "\n",
    "# Test the accuracy, precision, recall and F1 score of the model\n",
    "true_negative = 0\n",
    "false_negative = 0\n",
    "true_positive = 0\n",
    "false_positive = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (features, labels) in enumerate(test_loader):\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        for j in range(len(labels)):\n",
    "            if labels[j] == 0 and predicted[j] == 0:\n",
    "                true_negative += 1\n",
    "            elif labels[j] == 1 and predicted[j] == 0:\n",
    "                false_negative += 1\n",
    "            elif labels[j] == 1 and predicted[j] == 1:\n",
    "                true_positive += 1\n",
    "            elif labels[j] == 0 and predicted[j] == 1:\n",
    "                false_positive += 1\n",
    "\n",
    "print('Test set accuracy:    %d %%' % (100 * (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)))\n",
    "print('Test set precision:   %d %%' % (100 * true_positive / (true_positive + false_positive)))\n",
    "print('Test set recall:      %d %%' % (100 * true_positive / (true_positive + false_negative)))\n",
    "print('Test set F1 score     %d %%' % (100 * 2 * true_positive / (2 * true_positive + false_positive + false_negative)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6572afa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected 2D or 3D input (got 1D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/vera/projects/masters_project/scripts/Simple Classifier.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadoka/home/vera/projects/masters_project/scripts/Simple%20Classifier.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadoka/home/vera/projects/masters_project/scripts/Simple%20Classifier.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m residue \u001b[39min\u001b[39;00m test_protein[\u001b[39m0\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadoka/home/vera/projects/masters_project/scripts/Simple%20Classifier.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39m#residue = torch.tensor(residue).cuda()\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadoka/home/vera/projects/masters_project/scripts/Simple%20Classifier.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39m#residue = residue.view(1, 1, 768)\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmadoka/home/vera/projects/masters_project/scripts/Simple%20Classifier.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         output \u001b[39m=\u001b[39m model(residue)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadoka/home/vera/projects/masters_project/scripts/Simple%20Classifier.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(output\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadoka/home/vera/projects/masters_project/scripts/Simple%20Classifier.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         predicted_labels\u001b[39m.\u001b[39mappend(predicted\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39mRMSD_THRESHOLD)\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/vera/projects/masters_project/scripts/Simple Classifier.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadoka/home/vera/projects/masters_project/scripts/Simple%20Classifier.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmadoka/home/vera/projects/masters_project/scripts/Simple%20Classifier.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatchnorm1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadoka/home/vera/projects/masters_project/scripts/Simple%20Classifier.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmadoka/home/vera/projects/masters_project/scripts/Simple%20Classifier.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_input_dim(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39m\n\u001b[1;32m    142\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39m\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:301\u001b[0m, in \u001b[0;36mBatchNorm1d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_input_dim\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    300\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m--> 301\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mexpected 2D or 3D input (got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39mD input)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim())\n\u001b[1;32m    303\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: expected 2D or 3D input (got 1D input)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i in range(10):\n",
    "    # Add subplot vertically\n",
    "    ax = fig.add_subplot(10, 1, i+1)\n",
    "\n",
    "    # Test for a single protein\n",
    "    test_protein = dataset_orig[random.randint(0, len(dataset_orig))]\n",
    "\n",
    "    # Get the predicted labels for each residue\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for residue in test_protein[0]:\n",
    "            #residue = torch.tensor(residue).cuda()\n",
    "            #residue = residue.view(1, 1, 768)\n",
    "            output = model(residue)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            predicted_labels.append(predicted.cpu().numpy()[0]*RMSD_THRESHOLD)\n",
    "\n",
    "    # Get the actual labels for each residue\n",
    "    actual_labels = []\n",
    "    correct_labels = []\n",
    "    for residue in test_protein[1]:\n",
    "        actual_labels.append(residue)\n",
    "        correct_labels.append(int(residue >= RMSD_THRESHOLD))\n",
    "\n",
    "    # Get a list of correctly predicted labels\n",
    "    correctly_predicted_labels = []\n",
    "    for i in range(len(predicted_labels)):\n",
    "        if correct_labels[i] == 1 and predicted_labels[i] == RMSD_THRESHOLD:\n",
    "            correctly_predicted_labels.append(RMSD_THRESHOLD)\n",
    "        elif correct_labels[i] == 0 and predicted_labels[i] == 0:\n",
    "            correctly_predicted_labels.append(0)\n",
    "        else:\n",
    "            correctly_predicted_labels.append(None)\n",
    "\n",
    "    # Plot the actual labels as black\n",
    "    ax.plot(actual_labels, color='black')\n",
    "    ax.plot(predicted_labels, color='red', marker='o', linestyle='None')\n",
    "    ax.plot(correctly_predicted_labels, color='green', marker='o', linestyle='None')\n",
    "    ax.set_ylim(0, RMSD_THRESHOLD*5)\n",
    "    # Fill under the curve in a rainbow gradient\n",
    "    ax.fill_between(range(len(actual_labels)), actual_labels, color='black', alpha=0.2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69fd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62d2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
