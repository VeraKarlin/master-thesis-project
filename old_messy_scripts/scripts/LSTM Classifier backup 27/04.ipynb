{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os\n",
      "glob\n",
      "torch\n",
      "nn\n",
      "Dataset\n",
      "DataLoader\n",
      "literal_eval\n",
      "pd\n",
      "np\n",
      "plt\n",
      "itertools\n",
      "random\n",
      "Variable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"os\")\n",
    "import glob\n",
    "print(\"glob\")\n",
    "import torch\n",
    "print(\"torch\")\n",
    "from torch import nn\n",
    "print(\"nn\")\n",
    "from torch.utils.data import Dataset\n",
    "print(\"Dataset\")\n",
    "from torch.utils.data import DataLoader\n",
    "print(\"DataLoader\")\n",
    "from ast import literal_eval\n",
    "print(\"literal_eval\")\n",
    "import pandas as pd\n",
    "print(\"pd\")\n",
    "import numpy as np\n",
    "print(\"np\")\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"plt\")\n",
    "import itertools\n",
    "print(\"itertools\")\n",
    "import random\n",
    "print(\"random\")\n",
    "from torch.autograd import Variable\n",
    "print(\"Variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        # Only select the first 2000 rows for now\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature_path = os.path.join(self.root_dir, self.labels.iloc[idx,0])\n",
    "        embeddings = torch.load(feature_path)\n",
    "        features = embeddings[:, :, :768]\n",
    "        maps = embeddings[:, :, 768:]\n",
    "        rmsds = [float(label) for label in literal_eval(self.labels.iloc[idx, 1])]\n",
    "        if self.transform:\n",
    "            embedding = self.transform(embedding)\n",
    "        return features, maps, rmsds, feature_path\n",
    "    \n",
    "dataset = Dataset(csv_file='/home/vera/projects/masters_project/data/rmsd_dataset.csv',\n",
    "                                    root_dir='/home/vera/projects/masters_project/data/s-pred_features/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    csv_file_df = pd.read_csv('/home/vera/projects/masters_project/data/rmsd_dataset.csv')\n",
    "\n",
    "    # Test if the labels have the sane length as the embeddings\n",
    "    for i in range(len(dataset)):\n",
    "        features, maps, rmsds, feature_path = dataset[i]\n",
    "        if features.shape[1] != len(rmsds):\n",
    "            print('ERROR: The embedding and label for \"' + feature_path + '\" have different lengths!')\n",
    "            print('Embedding length: ' + str(features.shape[1]))\n",
    "            print('Label length: ' + str(len(rmsds)))\n",
    "\n",
    "            csv_file_df.drop(csv_file_df[csv_file_df['Unnamed: 0'] == feature_path.split('/')[-1]].index, inplace=True)\n",
    "\n",
    "    # Save the new csv file\n",
    "    csv_file_df.to_csv('/home/vera/projects/masters_project/data/rmsd_dataset.csv', index=False)\n",
    "\n",
    "print('False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0627,  0.4644,  0.3815,  ..., -0.2154, -0.4263, -0.8542],\n",
       "          [-0.2771, -0.5295,  0.3261,  ..., -0.5385,  0.6916, -0.0551],\n",
       "          [-0.6896, -0.8251,  0.2244,  ..., -0.6500,  0.4918, -0.6983],\n",
       "          ...,\n",
       "          [-0.6249, -1.5928, -0.3101,  ..., -1.4699, -0.1669, -1.6153],\n",
       "          [-0.9852, -0.8269,  0.4922,  ..., -0.5268,  0.0339, -1.4123],\n",
       "          [-0.7165, -0.3634,  0.1148,  ..., -0.0383,  0.6593, -0.9017]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[0.0021, 0.0031, 0.0012,  ..., 0.0023, 0.0022, 0.0028],\n",
       "          [0.0024, 0.0024, 0.0031,  ..., 0.0026, 0.0026, 0.0028],\n",
       "          [0.0022, 0.0029, 0.0031,  ..., 0.0027, 0.0028, 0.0029],\n",
       "          ...,\n",
       "          [0.0015, 0.0030, 0.0047,  ..., 0.0026, 0.0026, 0.0029],\n",
       "          [0.0011, 0.0038, 0.0029,  ..., 0.0027, 0.0025, 0.0029],\n",
       "          [0.0010, 0.0017, 0.0016,  ..., 0.0023, 0.0021, 0.0027]]],\n",
       "        device='cuda:0'),\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.02,\n",
       "  0.109,\n",
       "  0.154,\n",
       "  1.496,\n",
       "  1.546,\n",
       "  1.588,\n",
       "  1.581,\n",
       "  1.586,\n",
       "  1.547,\n",
       "  1.354,\n",
       "  1.126,\n",
       "  1.137,\n",
       "  1.133,\n",
       "  1.061,\n",
       "  1.041,\n",
       "  0.971,\n",
       "  0.376,\n",
       "  0.339,\n",
       "  0.28,\n",
       "  0.275,\n",
       "  0.264,\n",
       "  0.264,\n",
       "  0.363,\n",
       "  0.44,\n",
       "  0.477,\n",
       "  0.464,\n",
       "  0.56,\n",
       "  0.774,\n",
       "  0.724,\n",
       "  0.791,\n",
       "  0.906,\n",
       "  0.925,\n",
       "  0.882,\n",
       "  0.794,\n",
       "  0.826,\n",
       "  0.785,\n",
       "  0.42,\n",
       "  0.392,\n",
       "  0.43,\n",
       "  0.363,\n",
       "  0.17,\n",
       "  0.1,\n",
       "  0.123,\n",
       "  0.17,\n",
       "  0.177,\n",
       "  0.19,\n",
       "  0.218,\n",
       "  0.25,\n",
       "  0.374,\n",
       "  0.492,\n",
       "  0.638,\n",
       "  0.647,\n",
       "  0.48,\n",
       "  0.348,\n",
       "  0.277,\n",
       "  0.313,\n",
       "  0.269,\n",
       "  0.325,\n",
       "  0.346,\n",
       "  0.338,\n",
       "  0.303,\n",
       "  0.304,\n",
       "  0.317,\n",
       "  0.341,\n",
       "  0.326,\n",
       "  0.362,\n",
       "  0.344,\n",
       "  0.328,\n",
       "  0.323,\n",
       "  0.282,\n",
       "  0.286,\n",
       "  0.28,\n",
       "  0.267,\n",
       "  0.163,\n",
       "  0.15,\n",
       "  0.144,\n",
       "  0.13,\n",
       "  0.089,\n",
       "  0.074,\n",
       "  0.069,\n",
       "  0.089,\n",
       "  0.112,\n",
       "  0.158,\n",
       "  0.167,\n",
       "  0.183,\n",
       "  0.207,\n",
       "  0.251,\n",
       "  0.339,\n",
       "  0.377,\n",
       "  0.417,\n",
       "  0.414,\n",
       "  0.328,\n",
       "  0.291,\n",
       "  0.311,\n",
       "  0.329,\n",
       "  0.326,\n",
       "  0.211,\n",
       "  0.199,\n",
       "  0.188,\n",
       "  0.184,\n",
       "  0.183,\n",
       "  0.185,\n",
       "  0.159,\n",
       "  0.149,\n",
       "  0.174,\n",
       "  0.154,\n",
       "  0.175,\n",
       "  0.174,\n",
       "  0.18,\n",
       "  0.186,\n",
       "  0.262,\n",
       "  0.28,\n",
       "  0.281,\n",
       "  0.275,\n",
       "  0.306,\n",
       "  0.323,\n",
       "  0.314,\n",
       "  0.26,\n",
       "  0.293,\n",
       "  0.358,\n",
       "  0.358,\n",
       "  0.32,\n",
       "  0.217,\n",
       "  0.165,\n",
       "  0.228,\n",
       "  0.233,\n",
       "  0.211,\n",
       "  0.214,\n",
       "  0.215,\n",
       "  0.359,\n",
       "  0.54,\n",
       "  0.56,\n",
       "  0.615,\n",
       "  0.609,\n",
       "  0.605,\n",
       "  0.622,\n",
       "  0.601,\n",
       "  0.596,\n",
       "  0.476,\n",
       "  0.318,\n",
       "  0.198,\n",
       "  0.188,\n",
       "  0.173,\n",
       "  0.23,\n",
       "  0.263,\n",
       "  0.248,\n",
       "  0.295,\n",
       "  0.345,\n",
       "  0.373,\n",
       "  0.485,\n",
       "  0.574,\n",
       "  3.04,\n",
       "  3.293,\n",
       "  4.076,\n",
       "  4.092,\n",
       "  1.599,\n",
       "  1.667,\n",
       "  2.108,\n",
       "  2.65,\n",
       "  2.815,\n",
       "  2.685,\n",
       "  2.553,\n",
       "  2.424,\n",
       "  2.005,\n",
       "  1.762,\n",
       "  1.954,\n",
       "  1.217,\n",
       "  3.696,\n",
       "  2.985,\n",
       "  3.007,\n",
       "  2.849,\n",
       "  2.696,\n",
       "  2.16,\n",
       "  0.525,\n",
       "  0.638,\n",
       "  0.854,\n",
       "  0.89,\n",
       "  0.575,\n",
       "  0.355,\n",
       "  0.222,\n",
       "  0.158,\n",
       "  0.136,\n",
       "  0.147,\n",
       "  0.197,\n",
       "  0.414,\n",
       "  0.605,\n",
       "  0.738,\n",
       "  0.824,\n",
       "  0.852,\n",
       "  0.889,\n",
       "  1.022,\n",
       "  0.999,\n",
       "  1.034,\n",
       "  1.2,\n",
       "  1.07,\n",
       "  1.124,\n",
       "  1.034,\n",
       "  0.624,\n",
       "  0.69,\n",
       "  0.311,\n",
       "  0.341,\n",
       "  0.423,\n",
       "  0.441,\n",
       "  0.305,\n",
       "  0.305,\n",
       "  0.323,\n",
       "  0.5,\n",
       "  0.68,\n",
       "  0.611,\n",
       "  0.492,\n",
       "  0.381,\n",
       "  0.272,\n",
       "  0.257,\n",
       "  0.211,\n",
       "  0.19,\n",
       "  0.173,\n",
       "  0.141,\n",
       "  0.114,\n",
       "  0.078,\n",
       "  0.083,\n",
       "  0.107,\n",
       "  0.175,\n",
       "  0.219,\n",
       "  0.391,\n",
       "  0.635,\n",
       "  0.935,\n",
       "  1.497,\n",
       "  3.459,\n",
       "  5.007,\n",
       "  5.832,\n",
       "  5.559,\n",
       "  1.233,\n",
       "  0.355,\n",
       "  0.348,\n",
       "  0.025,\n",
       "  0.029,\n",
       "  0.018,\n",
       "  0.019,\n",
       "  0.02,\n",
       "  0.015,\n",
       "  0.011,\n",
       "  0.012,\n",
       "  0.017,\n",
       "  3.637,\n",
       "  3.198,\n",
       "  3.479,\n",
       "  2.905,\n",
       "  2.184,\n",
       "  1.078,\n",
       "  1.488,\n",
       "  0.809,\n",
       "  0.683,\n",
       "  0.477,\n",
       "  0.388,\n",
       "  0.103,\n",
       "  0.037,\n",
       "  0.04,\n",
       "  0.043,\n",
       "  0.039,\n",
       "  0.034,\n",
       "  0.034,\n",
       "  0.035,\n",
       "  0.036,\n",
       "  0.036,\n",
       "  0.031,\n",
       "  0.026,\n",
       "  0.031,\n",
       "  0.028,\n",
       "  0.028,\n",
       "  0.041,\n",
       "  0.042,\n",
       "  0.041,\n",
       "  0.038,\n",
       "  0.042,\n",
       "  0.037,\n",
       "  0.038,\n",
       "  0.041,\n",
       "  0.034,\n",
       "  0.035,\n",
       "  0.031,\n",
       "  0.032,\n",
       "  0.038,\n",
       "  0.041,\n",
       "  0.038,\n",
       "  0.035,\n",
       "  0.032,\n",
       "  0.033,\n",
       "  0.038,\n",
       "  0.042,\n",
       "  0.047,\n",
       "  0.057,\n",
       "  0.066,\n",
       "  0.075,\n",
       "  0.07,\n",
       "  0.063,\n",
       "  0.061,\n",
       "  0.035,\n",
       "  0.028,\n",
       "  0.029,\n",
       "  0.035,\n",
       "  0.036,\n",
       "  0.034,\n",
       "  0.03,\n",
       "  0.029,\n",
       "  0.03,\n",
       "  0.036,\n",
       "  0.042,\n",
       "  0.056,\n",
       "  0.091,\n",
       "  0.109,\n",
       "  0.117,\n",
       "  0.116,\n",
       "  0.124,\n",
       "  0.14,\n",
       "  0.141,\n",
       "  0.151,\n",
       "  0.197,\n",
       "  0.295,\n",
       "  0.295,\n",
       "  0.295,\n",
       "  0.295,\n",
       "  0.285,\n",
       "  0.263,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " '/home/vera/projects/masters_project/data/s-pred_features/embeddings_4kmu_a.pt')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8304 1038 1038\n",
      "torch.Size([1, 1, 116, 768])\n",
      "torch.Size([1, 1, 116, 288])\n",
      "116\n",
      "('/home/vera/projects/masters_project/data/s-pred_features/embeddings_4mpl_a.pt',)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train, validation and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "# Test if the dataset is split correctly\n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 1 \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Test if the dataloaders are working correctly\n",
    "for i, (features, maps, rmsds, feature_path) in enumerate(train_loader):\n",
    "    print(features.shape)\n",
    "    print(maps.shape)\n",
    "    print(len(rmsds))\n",
    "    print(feature_path)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LSTM model\n",
    "class lstm_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_feature_size=768, hidden_node=256, dropout=0.25, class_num=8):\n",
    "        super(lstm_net, self).__init__()\n",
    "\n",
    "        self.linear_proj = nn.Sequential(\n",
    "            nn.Linear(input_feature_size, input_feature_size // 2),\n",
    "            nn.InstanceNorm1d(input_feature_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_feature_size // 2, input_feature_size // 4),\n",
    "            nn.InstanceNorm1d(input_feature_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_feature_size // 4, input_feature_size // 4),\n",
    "        )\n",
    "\n",
    "        lstm_input_feature_size = input_feature_size // 4 + 144*2\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_feature_size,\n",
    "            hidden_size=hidden_node,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.to_property = nn.Sequential(\n",
    "            nn.Linear(hidden_node * 2, hidden_node * 2),\n",
    "            nn.InstanceNorm1d(hidden_node * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_node * 2, class_num),\n",
    "        )\n",
    "\n",
    "    def forward(self, msa_query_embeddings, msa_attention_features):\n",
    "        msa_query_embeddings = self.linear_proj(msa_query_embeddings)\n",
    "\n",
    "        lstm_input = torch.cat([msa_query_embeddings, msa_attention_features], dim=2)\n",
    "        lstm_input = lstm_input.permute((1, 0, 2))\n",
    "\n",
    "        lstm_output, lstm_hidden = self.lstm(lstm_input)\n",
    "        lstm_output = lstm_output.permute((1, 0, 2))\n",
    "        \n",
    "        label_output = self.to_property(lstm_output)\n",
    "        #print(lstm_output)\n",
    "        #print(label_output)\n",
    "\n",
    "        return label_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 400, 768] at entry 0 and [1, 389, 768] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m valid_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     36\u001b[0m valid_acc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 38\u001b[0m \u001b[39mfor\u001b[39;00m i, (features, maps, rmsds, feature_path) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     39\u001b[0m     features \u001b[39m=\u001b[39m features[\u001b[39m0\u001b[39m,:,:,:]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m     maps \u001b[39m=\u001b[39m maps[\u001b[39m0\u001b[39m,:,:,:]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 400, 768] at entry 0 and [1, 389, 768] at entry 1"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0\n",
    "#WEIGHT_DECAY = 0.0001\n",
    "BATCH_SIZE = 16\n",
    "HIDDEN_NODE = 256\n",
    "DROPOUT = 0\n",
    "CLASS_NUM = 1\n",
    "\n",
    "RMSD_THRESHOLD = 0.5\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "model = lstm_net(input_feature_size=768, hidden_node=HIDDEN_NODE, dropout=DROPOUT, class_num=CLASS_NUM)\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "train_acc_list = []\n",
    "valid_acc_list = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.zero_grad()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "\n",
    "    for i, (features, maps, rmsds, feature_path) in enumerate(train_loader):\n",
    "        features = features[0,:,:,:].to(device)\n",
    "        maps = maps[0,:,:,:].to(device)\n",
    "        labels = torch.Tensor([int(label >= RMSD_THRESHOLD) for label in rmsds]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(msa_query_embeddings=features, msa_attention_features=maps)\n",
    "        loss = criterion(output[0,:,0], labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (output.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_loss_list.append(train_loss / len(train_loader))\n",
    "    train_acc_list.append(train_acc / len(train_dataset))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (features, maps, rmsds, feature_path) in enumerate(valid_loader):\n",
    "            features = features[0,:,:,:].to(device)\n",
    "            maps = maps[0,:,:,:].to(device)\n",
    "            labels = torch.Tensor([int(label >= RMSD_THRESHOLD) for label in rmsds]).to(device)\n",
    "\n",
    "            output = model(msa_query_embeddings=features, msa_attention_features=maps)\n",
    "            loss = criterion(output[0,:,0], labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            valid_acc += (output.argmax(1) == labels).sum().item()\n",
    "\n",
    "        valid_loss_list.append(valid_loss / len(valid_loader))\n",
    "        valid_acc_list.append(valid_acc / len(valid_dataset))\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTraining Accuracy: {:.6f} \\tValidation Accuracy: {:.6f}'.format(\n",
    "        epoch, train_loss_list[-1], valid_loss_list[-1], train_acc_list[-1], valid_acc_list[-1]))\n",
    "    \n",
    "    model.train()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE3-nvidia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
