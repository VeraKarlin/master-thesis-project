{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6239dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pathlib\n",
    "import string\n",
    "\n",
    "import torch\n",
    "\n",
    "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, pretrained, MSATransformer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "import itertools\n",
    "from typing import List, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_insertions(sequence: str) -> str:\n",
    "    \"\"\" Removes any insertions into the sequence. Needed to load aligned sequences in an MSA. \"\"\"\n",
    "    # This is an efficient way to delete lowercase characters and insertion characters from a string\n",
    "    deletekeys = dict.fromkeys(string.ascii_lowercase)\n",
    "    deletekeys[\".\"] = None\n",
    "    deletekeys[\"*\"] = None\n",
    "\n",
    "    translation = str.maketrans(deletekeys)\n",
    "    return sequence.translate(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_msa(filename: str, nseq: int) -> List[Tuple[str, str]]:\n",
    "    \"\"\" Reads the first nseq sequences from an MSA file, automatically removes insertions.\n",
    "    \n",
    "    The input file must be in a3m format (although we use the SeqIO fasta parser)\n",
    "    for remove_insertions to work properly.\"\"\"\n",
    "\n",
    "    msa = [\n",
    "        (record.description, remove_insertions(str(record.seq)))\n",
    "        for record in itertools.islice(SeqIO.parse(filename, \"fasta\"), nseq)\n",
    "    ]\n",
    "    return msa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c99be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Label a deep mutational scan with predictions from an ensemble of ESM-1v models.\"  # noqa\n",
    "    )\n",
    "\n",
    "    # fmt: off\n",
    "    parser.add_argument(\n",
    "        \"--model-location\",\n",
    "        type=str,\n",
    "        help=\"PyTorch model file OR name of pretrained model to download (see README for models)\",\n",
    "        nargs=\"+\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sequence\",\n",
    "        type=str,\n",
    "        help=\"Base sequence to which mutations were applied\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dms-input\",\n",
    "        type=pathlib.Path,\n",
    "        help=\"CSV file containing the deep mutational scan\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mutation-col\",\n",
    "        type=str,\n",
    "        default=\"mutant\",\n",
    "        help=\"column in the deep mutational scan labeling the mutation as 'AiB'\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dms-output\",\n",
    "        type=pathlib.Path,\n",
    "        help=\"Output file containing the deep mutational scan along with predictions\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--offset-idx\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Offset of the mutation positions in `--mutation-col`\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--scoring-strategy\",\n",
    "        type=str,\n",
    "        default=\"wt-marginals\",\n",
    "        choices=[\"wt-marginals\", \"pseudo-ppl\", \"masked-marginals\"],\n",
    "        help=\"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--msa-path\",\n",
    "        type=pathlib.Path,\n",
    "        help=\"path to MSA in a3m format (required for MSA Transformer)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--msa-samples\",\n",
    "        type=int,\n",
    "        default=400,\n",
    "        help=\"number of sequences to select from the start of the MSA\"\n",
    "    )\n",
    "    # fmt: on\n",
    "    parser.add_argument(\"--nogpu\", action=\"store_true\", help=\"Do not use GPU even if available\")\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_row(row, sequence, token_probs, alphabet, offset_idx):\n",
    "    wt, idx, mt = row[0], int(row[1:-1]) - offset_idx, row[-1]\n",
    "    assert sequence[idx] == wt, \"The listed wildtype does not match the provided sequence\"\n",
    "\n",
    "    wt_encoded, mt_encoded = alphabet.get_idx(wt), alphabet.get_idx(mt)\n",
    "\n",
    "    # add 1 for BOS\n",
    "    score = token_probs[0, 1 + idx, mt_encoded] - token_probs[0, 1 + idx, wt_encoded]\n",
    "    return score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pppl(row, sequence, model, alphabet, offset_idx):\n",
    "    wt, idx, mt = row[0], int(row[1:-1]) - offset_idx, row[-1]\n",
    "    assert sequence[idx] == wt, \"The listed wildtype does not match the provided sequence\"\n",
    "\n",
    "    # modify the sequence\n",
    "    sequence = sequence[:idx] + mt + sequence[(idx + 1) :]\n",
    "\n",
    "    # encode the sequence\n",
    "    data = [\n",
    "        (\"protein1\", sequence),\n",
    "    ]\n",
    "\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "    wt_encoded, mt_encoded = alphabet.get_idx(wt), alphabet.get_idx(mt)\n",
    "\n",
    "    # compute probabilities at each position\n",
    "    log_probs = []\n",
    "    for i in range(1, len(sequence) - 1):\n",
    "        batch_tokens_masked = batch_tokens.clone()\n",
    "        batch_tokens_masked[0, i] = alphabet.mask_idx\n",
    "        with torch.no_grad():\n",
    "            token_probs = torch.log_softmax(model(batch_tokens_masked.cuda())[\"logits\"], dim=-1)\n",
    "        log_probs.append(token_probs[0, i, alphabet.get_idx(sequence[i])].item())  # vocab size\n",
    "    return sum(log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51aeac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # Load the deep mutational scan\n",
    "    df = pd.read_csv(args.dms_input)\n",
    "\n",
    "    # inference for each model\n",
    "    for model_location in args.model_location:\n",
    "        model, alphabet = pretrained.load_model_and_alphabet(model_location)\n",
    "        model.eval()\n",
    "        if torch.cuda.is_available() and not args.nogpu:\n",
    "            model = model.cuda()\n",
    "            print(\"Transferred model to GPU\")\n",
    "\n",
    "        batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "        if isinstance(model, MSATransformer):\n",
    "            data = [read_msa(args.msa_path, args.msa_samples)]\n",
    "            assert (\n",
    "                args.scoring_strategy == \"masked-marginals\"\n",
    "            ), \"MSA Transformer only supports masked marginal strategy\"\n",
    "\n",
    "            batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "            print(batch_tokens.shape)\n",
    "    #df.to_csv(args.dms_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad355975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred model to GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vera/projects/masters_project/esm-main/examples/variant-prediction/predict.py\", line 241, in <module>\n",
      "    main(args)\n",
      "  File \"/home/vera/projects/masters_project/esm-main/examples/variant-prediction/predict.py\", line 167, in main\n",
      "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
      "  File \"/home/vera/miniconda3/envs/SE3-nvidia/lib/python3.9/site-packages/esm/data.py\", line 327, in __call__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Received unaligned sequences for input to MSA, all sequence lengths must be equal.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"python predict.py \\\n",
    "    --model-location esm_msa1b_t12_100M_UR50S \\\n",
    "    --sequence HPETLVKVKDAEDQLGARVGYIELDLNSGKILESFRPEERFPMMSTFKVLLCGAVLSRVDAGQEQLGRRIHYSQNDLVEYSPVTEKHLTDGMTVRELCSAAITMSDNTAANLLLTTIGGPKELTAFLHNMGDHVTRLDRWEPELNEAIPNDERDTTMPAAMATTLRKLLTGELLTLASRQQLIDWMEADKVAGPLLRSALPAGWFIADKSGAGERGSRGIIAALGPDGKPSRIVVIYTTGSQATMDERNRQIAEIGASLIKHW \\\n",
    "    --dms-input ./data/BLAT_ECOLX_Ranganathan2015.csv \\\n",
    "    --mutation-col mutant \\\n",
    "    --dms-output ./data/BLAT_ECOLX_Ranganathan2015_labeled.csv \\\n",
    "    --offset-idx 24 \\\n",
    "    --scoring-strategy masked-marginals \\\n",
    "    --msa-path ./data/BLAT_ECOLX_1_b0.5.a3m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4e93d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vera/projects/masters_project/esm-main/examples/variant-prediction\n"
     ]
    }
   ],
   "source": [
    "cd '../esm-main/examples/variant-prediction/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f74ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57e81c5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mbatch_tokens\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'batch_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a871d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
